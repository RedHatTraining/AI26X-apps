{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Server Failure Prediction - Model Training\n",
    "\n",
    "This notebook trains a neural network to predict server failures within 48 hours based on system metrics.\n",
    "\n",
    "**Note:** This notebook contains intentional bugs for learning purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import data_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and test data\n",
    "raw_server_metrics = data_utils.load_data(\"server_metrics.csv\")\n",
    "\n",
    "# Preprocess features (including data scaling)\n",
    "X, y = data_utils.preprocess_features(raw_server_metrics)\n",
    "\n",
    "# Split data into train/set subsets\n",
    "X_train, X_test, y_train, y_test = data_utils.split_data(X, y)\n",
    "\n",
    "# Save prepared data for inspection\n",
    "data_utils.save_prepared_data(X_train, X_test, y_train, y_test)\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")\n",
    "\n",
    "print(\"\\nClass distribution in training set:\")\n",
    "print(y_train.value_counts())\n",
    "print(f\"\\nFailure rate: {y_train.mean():.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Handle Class Imbalance\n",
    "\n",
    "Server failures are rare (~16%), so use class weights to handle the imbalance.\n",
    "\n",
    "Class weights tell the model to pay more attention to the minority class (failures) during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class weights to handle imbalance\n",
    "class_weights = data_utils.calculate_class_weights(y_train)\n",
    "\n",
    "print(\n",
    "    \"\\nThe minority class (failures) should have \"\n",
    "    f\"{class_weights[1]:.1f}x more weight during training.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inspect Training Set Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect dimensions of training set (num_samples x num_features)\n",
    "X_num_samples = X_train.shape[0]\n",
    "print(\"\\nNumber of training samples:\")\n",
    "print(X_num_samples)\n",
    "\n",
    "# TODO. Fix BUG: Wrong input shape! Should be X_train.shape[1]\n",
    "X_num_features = 10\n",
    "print(\"\\nNumber of training features:\")\n",
    "print(X_num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Define Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define the network\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(X_num_features,)),\n",
    "    tf.keras.layers.Dense(50, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(10, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(int(2e7), activation='relu'), # TODO. Fix BUG: Layer is too large!\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=[\n",
    "        tf.keras.metrics.Precision(name='precision'),\n",
    "        tf.keras.metrics.Recall(name='recall')\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Setup TensorBoard Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup TensorBoard with descriptive logging\n",
    "log_dir = \"logs/neural_network\"\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=log_dir\n",
    ")\n",
    "\n",
    "print(f\"TensorBoard logs will be saved to: {log_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train the Model\n",
    "\n",
    "**Note:** This cell fails initially due to the bugs above. Fix them first.\n",
    "\n",
    "**Tip:** Open the Kernel Usage panel (right sidebar) to monitor memory usage during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=50,\n",
    "    batch_size=3000,\n",
    "    validation_split=0.2,\n",
    "    class_weight=class_weights,\n",
    "    callbacks=[tensorboard_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "test_loss, test_precision, test_recall = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Precision: {test_precision:.4f}\")\n",
    "print(f\"Test Recall: {test_recall:.4f}\")\n",
    "\n",
    "# Calculate F1 score\n",
    "f1_score = 2 * (test_precision * test_recall) / (test_precision + test_recall)\n",
    "print(f\"Test F1 Score: {f1_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Train a Second Model for Comparison\n",
    "\n",
    "After fixing the bugs, train a second model to compare.\n",
    "\n",
    "This model uses a simpler logistic regression architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple logistic regression model for comparison\n",
    "# Logistic regression = single layer with sigmoid activation\n",
    "model_v2 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_v2.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=[\n",
    "        tf.keras.metrics.Precision(name='precision'),\n",
    "        tf.keras.metrics.Recall(name='recall')\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Logistic Regression model\")\n",
    "model_v2.summary()\n",
    "\n",
    "# Setup TensorBoard for second model\n",
    "log_dir_v2 = \"logs/logistic_regression\"\n",
    "tensorboard_callback_v2 = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=log_dir_v2\n",
    ")\n",
    "\n",
    "print(\"\\nTraining model v2...\")\n",
    "history_v2 = model_v2.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=50,\n",
    "    batch_size=3000,\n",
    "    validation_split=0.2,\n",
    "    class_weight=class_weights,\n",
    "    callbacks=[tensorboard_callback]\n",
    ")\n",
    "\n",
    "print(\"\\nLogistic regression model trained. Compare both models in TensorBoard.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Compare Models\n",
    "\n",
    "Evaluate the second model and compare results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate second model on test set\n",
    "test_loss_v2, test_acc_v2, test_precision_v2, test_recall_v2 = model_v2.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print(\"Model Comparison:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Metric':<20} {'Model v1':<20} {'Model v2':<20}\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Test Precision':<20} {test_precision:.4f}{'':<16} {test_precision_v2:.4f}\")\n",
    "print(f\"{'Test Recall':<20} {test_recall:.4f}{'':<16} {test_recall_v2:.4f}\")\n",
    "\n",
    "# Calculate F1 scores\n",
    "f1_v1 = 2 * (test_precision * test_recall) / (test_precision + test_recall)\n",
    "f1_v2 = 2 * (test_precision_v2 * test_recall_v2) / (test_precision_v2 + test_recall_v2)\n",
    "print(f\"{'Test F1 Score':<20} {f1_v1:.4f}{'':<16} {f1_v2:.4f}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Calculate model sizes\n",
    "params_v1 = model.count_params()\n",
    "params_v2 = model_v2.count_params()\n",
    "print(f\"\\nModel v1 parameters: {params_v1:,}\")\n",
    "print(f\"Model v2 parameters: {params_v2:,}\")\n",
    "print(f\"Parameter reduction: {(1 - params_v2/params_v1)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Test Predictions on Sample Servers\n",
    "\n",
    "Let's test both models on specific server scenarios to see how they perform in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import preprocessing utility\n",
    "from data_utils import preprocess_for_inference\n",
    "\n",
    "# Test both models on sample cases\n",
    "test_cases = [\n",
    "    {\n",
    "        'desc': 'Healthy server',\n",
    "        'actual': 0,  # Did not fail\n",
    "        'metrics': {\n",
    "            'server_age_months': 12,\n",
    "            'cpu_temp_celsius': 55.0,\n",
    "            'cpu_utilization_percent': 45.0,\n",
    "            'memory_usage_percent': 60.0,\n",
    "            'disk_io_ops_per_sec': 500,\n",
    "            'network_throughput_mbps': 300.0,\n",
    "            'fan_speed_rpm': 3000,\n",
    "            'power_draw_watts': 250.0,\n",
    "            'disk_read_errors_24h': 0,\n",
    "            'memory_errors_24h': 0,\n",
    "            'workload_type': 'web'\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'desc': 'Failed server',\n",
    "        'actual': 1,  # Failed\n",
    "        'metrics': {\n",
    "            'server_age_months': 48,\n",
    "            'cpu_temp_celsius': 85.0,\n",
    "            'cpu_utilization_percent': 92.0,\n",
    "            'memory_usage_percent': 88.0,\n",
    "            'disk_io_ops_per_sec': 1800,\n",
    "            'network_throughput_mbps': 850.0,\n",
    "            'fan_speed_rpm': 2100,\n",
    "            'power_draw_watts': 420.0,\n",
    "            'disk_read_errors_24h': 15,\n",
    "            'memory_errors_24h': 8,\n",
    "            'workload_type': 'database'\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'desc': 'Moderate load',\n",
    "        'actual': 0,  # Did not fail\n",
    "        'metrics': {\n",
    "            'server_age_months': 30,\n",
    "            'cpu_temp_celsius': 72.0,\n",
    "            'cpu_utilization_percent': 75.0,\n",
    "            'memory_usage_percent': 78.0,\n",
    "            'disk_io_ops_per_sec': 1200,\n",
    "            'network_throughput_mbps': 550.0,\n",
    "            'fan_speed_rpm': 2600,\n",
    "            'power_draw_watts': 320.0,\n",
    "            'disk_read_errors_24h': 5,\n",
    "            'memory_errors_24h': 2,\n",
    "            'workload_type': 'compute'\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"{'Case':<20} {'Actual':<10} {'Model v1':<12} {'Model v2':<12} {'Winner':<10}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for case in test_cases:\n",
    "    # Preprocess the data using saved scaler from training\n",
    "    sample = pd.DataFrame([case['metrics']])\n",
    "    sample_processed = preprocess_for_inference(sample)\n",
    "\n",
    "    pred_v1 = model.predict(sample_processed, verbose=0)[0][0]\n",
    "    pred_v2 = model_v2.predict(sample_processed, verbose=0)[0][0]\n",
    "\n",
    "    # Determine predictions (>0.5 = failure)\n",
    "    pred_v1_class = int(pred_v1 > 0.5)\n",
    "    pred_v2_class = int(pred_v2 > 0.5)\n",
    "\n",
    "    # Who got it right?\n",
    "    v1_correct = \"✓\" if pred_v1_class == case['actual'] else \"✗\"\n",
    "    v2_correct = \"✓\" if pred_v2_class == case['actual'] else \"✗\"\n",
    "\n",
    "    if pred_v1_class == case['actual'] and pred_v2_class != case['actual']:\n",
    "        winner = \"Model v1\"\n",
    "    elif pred_v2_class == case['actual'] and pred_v1_class != case['actual']:\n",
    "        winner = \"Model v2\"\n",
    "    elif pred_v1_class == case['actual'] and pred_v2_class == case['actual']:\n",
    "        winner = \"Both\"\n",
    "    else:\n",
    "        winner = \"Neither\"\n",
    "\n",
    "    actual_str = \"Fail\" if case['actual'] == 1 else \"OK\"\n",
    "    print(f\"{case['desc']:<20} {actual_str:<10} {v1_correct} ({pred_v1:.2f}){'':<6} {v2_correct} ({pred_v2:.2f}){'':<6} {winner:<10}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Launch TensorBoard\n",
    "\n",
    "Complete and run the cells below to visualize training metrics in TensorBoard.\n",
    "\n",
    "**Comparing models in TensorBoard:**\n",
    "- Both models log to separate directories: `neural_network` and `logistic_regression`\n",
    "- In TensorBoard's left sidebar, ensure both runs are checked (selected)\n",
    "- The SCALARS tab can overlay both models on the same graph for easy comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load TensorBoard extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Set the TensorBoard proxy URL for RHOAI workbench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Launch TensorBoard"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
