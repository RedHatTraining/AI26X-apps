{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Server Failure Prediction - Model Training\n",
    "\n",
    "This notebook trains a neural network to predict server failures within 48 hours based on system metrics.\n",
    "\n",
    "**Note:** This notebook contains intentional bugs for learning purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Prepared Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and test data\n",
    "train_df = pd.read_csv('train_data.csv')\n",
    "test_df = pd.read_csv('test_data.csv')\n",
    "\n",
    "print(f\"Training set shape: {train_df.shape}\")\n",
    "print(f\"Test set shape: {test_df.shape}\")\n",
    "\n",
    "# Separate features and target\n",
    "X_train = train_df.drop('failure_within_48h', axis=1)\n",
    "y_train = train_df['failure_within_48h']\n",
    "\n",
    "X_test = test_df.drop('failure_within_48h', axis=1)\n",
    "y_test = test_df['failure_within_48h']\n",
    "\n",
    "print(f\"\\nClass distribution in training set:\")\n",
    "print(y_train.value_counts())\n",
    "print(f\"\\nFailure rate: {y_train.mean():.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Handle Class Imbalance\n",
    "\n",
    "Server failures are rare (~5%), so we balance the classes for better model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate majority and minority classes\n",
    "train_majority = train_df[train_df['failure_within_48h'] == 0]\n",
    "train_minority = train_df[train_df['failure_within_48h'] == 1]\n",
    "\n",
    "print(f\"Majority class samples: {len(train_majority)}\")\n",
    "print(f\"Minority class samples: {len(train_minority)}\")\n",
    "\n",
    "# Upsample minority class to match majority\n",
    "train_minority_upsampled = resample(\n",
    "    train_minority,\n",
    "    replace=True,\n",
    "    n_samples=len(train_majority),\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Upsampled minority class: {len(train_minority_upsampled)} samples\")\n",
    "\n",
    "# Combine majority and upsampled minority\n",
    "train_balanced = pd.concat([train_majority, train_minority_upsampled])\n",
    "\n",
    "print(f\"Balanced training set: {len(train_balanced)} samples\")\n",
    "\n",
    "# Shuffle the data\n",
    "train_balanced = train_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Separate features and target again\n",
    "X_train_balanced = train_balanced.drop('failure_within_48h', axis=1)\n",
    "y_train_balanced = train_balanced['failure_within_48h']\n",
    "\n",
    "print(\"\\nBalanced class distribution:\")\n",
    "print(y_train_balanced.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inspect Training Set Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect dimensions of training set (num_samples x num_features)\n",
    "X_num_samples = X_train_balanced.shape[0]\n",
    "print(\"\\nNumber of training samples:\")\n",
    "print(X_num_samples)\n",
    "\n",
    "# TODO. Fix BUG: Wrong input shape! Should be X_train_balanced.shape[1]\n",
    "X_num_features = 10\n",
    "print(\"\\nNumber of training features:\")\n",
    "print(X_num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Define Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define the network\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(X_num_features,)),\n",
    "    tf.keras.layers.Dense(10, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(5, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(int(2e7), activation='relu'), # TODO. Fix BUG: Layer is too large!\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',  # BUG: Wrong loss function!\n",
    "    metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Setup TensorBoard Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create log directory with timestamp\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# Early stopping to prevent overfitting\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "print(f\"TensorBoard logs will be saved to: {log_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train the Model\n",
    "\n",
    "**Note:** This cell will fail due to the bugs above. Fix them first!\n",
    "\n",
    "**Tip:** Open the Kernel Usage panel (right sidebar) to monitor memory usage during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train_balanced,\n",
    "    y_train_balanced,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[tensorboard_callback, early_stopping],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "test_loss, test_acc, test_precision, test_recall = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"Test Precision: {test_precision:.4f}\")\n",
    "print(f\"Test Recall: {test_recall:.4f}\")\n",
    "\n",
    "# Calculate F1 score\n",
    "f1_score = 2 * (test_precision * test_recall) / (test_precision + test_recall)\n",
    "print(f\"Test F1 Score: {f1_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Launch TensorBoard\n",
    "\n",
    "Run this cell to visualize training metrics in TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load TensorBoard extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Set the TensorBoard proxy URL for RHOAI workbench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Launch TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Train a Second Model for Comparison\n",
    "\n",
    "After fixing the bugs, train a second model with different architecture to compare in TensorBoard.\n",
    "\n",
    "This model uses a deeper architecture with more layers but appropriately sized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a simple logistic regression model for comparison\n",
    "# Logistic regression = single layer with sigmoid activation\n",
    "model_v2 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(X_train_balanced.shape[1],)),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_v2.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
    ")\n",
    "\n",
    "print(\"Model v2 architecture\")\n",
    "model_v2.summary()\n",
    "\n",
    "# Setup TensorBoard for second model\n",
    "log_dir_v2 = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"_v2_logreg\"\n",
    "tensorboard_callback_v2 = tf.keras.callbacks.TensorBoard(log_dir=log_dir_v2, histogram_freq=1)\n",
    "\n",
    "# Train second model\n",
    "print(\"\\nTraining model v2...\")\n",
    "history_v2 = model_v2.fit(\n",
    "    X_train_balanced,\n",
    "    y_train_balanced,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[tensorboard_callback_v2, early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nLogistic regression model trained! Compare both models in TensorBoard.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Compare Models\n",
    "\n",
    "Evaluate the second model and compare results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate second model on test set\n",
    "test_loss_v2, test_acc_v2, test_precision_v2, test_recall_v2 = model_v2.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print(\"Model Comparison:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Metric':<20} {'Model v1':<20} {'Model v2':<20}\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Test Accuracy':<20} {test_acc:.4f}{'':<16} {test_acc_v2:.4f}\")\n",
    "print(f\"{'Test Precision':<20} {test_precision:.4f}{'':<16} {test_precision_v2:.4f}\")\n",
    "print(f\"{'Test Recall':<20} {test_recall:.4f}{'':<16} {test_recall_v2:.4f}\")\n",
    "\n",
    "# Calculate F1 scores\n",
    "f1_v1 = 2 * (test_precision * test_recall) / (test_precision + test_recall)\n",
    "f1_v2 = 2 * (test_precision_v2 * test_recall_v2) / (test_precision_v2 + test_recall_v2)\n",
    "print(f\"{'Test F1 Score':<20} {f1_v1:.4f}{'':<16} {f1_v2:.4f}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Calculate model sizes\n",
    "params_v1 = model.count_params()\n",
    "params_v2 = model_v2.count_params()\n",
    "print(f\"\\nModel v1 parameters: {params_v1:,}\")\n",
    "print(f\"Model v2 parameters: {params_v2:,}\")\n",
    "print(f\"Parameter reduction: {(1 - params_v2/params_v1)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Test Predictions on Sample Servers\n",
    "\n",
    "Let's test both models on specific server scenarios to see how they perform in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample servers with clear characteristics\n",
    "# We need to match the same features as our training data\n",
    "\n",
    "# Get feature names (excluding target)\n",
    "feature_names = X_train.columns.tolist()\n",
    "print(f\"Features: {feature_names}\\n\")\n",
    "\n",
    "# Sample 1: HEALTHY server - normal metrics\n",
    "healthy_server = pd.DataFrame([{\n",
    "    'server_age_months': 12,\n",
    "    'cpu_temp_celsius': 55.0,\n",
    "    'cpu_utilization_percent': 45.0,\n",
    "    'memory_usage_percent': 60.0,\n",
    "    'disk_io_ops_per_sec': 500,\n",
    "    'network_throughput_mbps': 300.0,\n",
    "    'fan_speed_rpm': 3000,\n",
    "    'power_draw_watts': 250.0,\n",
    "    'disk_read_errors_24h': 0,\n",
    "    'memory_errors_24h': 0,\n",
    "    'workload_type_compute': 0,\n",
    "    'workload_type_database': 0,\n",
    "    'workload_type_storage': 0,\n",
    "    'workload_type_web': 1\n",
    "}])\n",
    "\n",
    "# Sample 2: AT-RISK server - high temperature, errors, old server\n",
    "at_risk_server = pd.DataFrame([{\n",
    "    'server_age_months': 48,\n",
    "    'cpu_temp_celsius': 85.0,\n",
    "    'cpu_utilization_percent': 92.0,\n",
    "    'memory_usage_percent': 88.0,\n",
    "    'disk_io_ops_per_sec': 1800,\n",
    "    'network_throughput_mbps': 850.0,\n",
    "    'fan_speed_rpm': 2100,\n",
    "    'power_draw_watts': 420.0,\n",
    "    'disk_read_errors_24h': 15,\n",
    "    'memory_errors_24h': 8,\n",
    "    'workload_type_compute': 0,\n",
    "    'workload_type_database': 1,\n",
    "    'workload_type_storage': 0,\n",
    "    'workload_type_web': 0\n",
    "}])\n",
    "\n",
    "# Sample 3: MODERATE-RISK server - some warning signs\n",
    "moderate_server = pd.DataFrame([{\n",
    "    'server_age_months': 30,\n",
    "    'cpu_temp_celsius': 72.0,\n",
    "    'cpu_utilization_percent': 75.0,\n",
    "    'memory_usage_percent': 78.0,\n",
    "    'disk_io_ops_per_sec': 1200,\n",
    "    'network_throughput_mbps': 550.0,\n",
    "    'fan_speed_rpm': 2600,\n",
    "    'power_draw_watts': 320.0,\n",
    "    'disk_read_errors_24h': 5,\n",
    "    'memory_errors_24h': 2,\n",
    "    'workload_type_compute': 1,\n",
    "    'workload_type_database': 0,\n",
    "    'workload_type_storage': 0,\n",
    "    'workload_type_web': 0\n",
    "}])\n",
    "\n",
    "# Make predictions\n",
    "def predict_and_explain(server_data, server_name):\n",
    "    \"\"\"Make predictions and display results.\"\"\"\n",
    "    pred_v1 = model.predict(server_data, verbose=0)[0][0]\n",
    "    pred_v2 = model_v2.predict(server_data, verbose=0)[0][0]\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"{server_name.upper()}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Display key metrics\n",
    "    print(f\"\\nServer Characteristics:\")\n",
    "    print(f\"  Age: {server_data['server_age_months'].values[0]} months\")\n",
    "    print(f\"  CPU Temp: {server_data['cpu_temp_celsius'].values[0]:.1f}°C\")\n",
    "    print(f\"  CPU Usage: {server_data['cpu_utilization_percent'].values[0]:.1f}%\")\n",
    "    print(f\"  Memory Usage: {server_data['memory_usage_percent'].values[0]:.1f}%\")\n",
    "    print(f\"  Fan Speed: {server_data['fan_speed_rpm'].values[0]} RPM\")\n",
    "    print(f\"  Disk Errors (24h): {server_data['disk_read_errors_24h'].values[0]}\")\n",
    "    print(f\"  Memory Errors (24h): {server_data['memory_errors_24h'].values[0]}\")\n",
    "    \n",
    "    # Display predictions\n",
    "    print(f\"\\nFailure Predictions (within 48 hours):\")\n",
    "    print(f\"  Model v1: {pred_v1*100:.1f}% probability\")\n",
    "    print(f\"  Model v2: {pred_v2*100:.1f}% probability\")\n",
    "    \n",
    "    # Interpretation\n",
    "    threshold = 0.5\n",
    "    risk_v1 = \"HIGH RISK\" if pred_v1 > threshold else \"Low risk\"\n",
    "    risk_v2 = \"HIGH RISK\" if pred_v2 > threshold else \"Low risk\"\n",
    "    \n",
    "    print(f\"\\nRisk Assessment (threshold = 50%):\")\n",
    "    print(f\"  Model v1: {risk_v1}\")\n",
    "    print(f\"  Model v2: {risk_v2}\")\n",
    "    \n",
    "    # Agreement\n",
    "    if (pred_v1 > threshold) == (pred_v2 > threshold):\n",
    "        print(f\"\\n✓ Both models agree on the risk level\")\n",
    "    else:\n",
    "        print(f\"\\n⚠ Models disagree - further investigation recommended\")\n",
    "\n",
    "# Run predictions\n",
    "predict_and_explain(healthy_server, \"Healthy Server\")\n",
    "predict_and_explain(at_risk_server, \"At-Risk Server (High Failure Probability)\")\n",
    "predict_and_explain(moderate_server, \"Moderate-Risk Server\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"\\nConclusion:\")\n",
    "print(f\"- Healthy servers show low failure probability (<10%)\")\n",
    "print(f\"- At-risk servers (high temp, errors, old) show high probability (>50%)\")\n",
    "print(f\"- Both models should identify similar risk patterns\")\n",
    "print(f\"- Model v2 may show slightly better calibration due to deeper architecture\")\n",
    "print(f\"{'='*70}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
