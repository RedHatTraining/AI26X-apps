{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Export PyTorch Model to ONNX Format\n",
    "\n",
    "This notebook demonstrates how to export a pre-trained PyTorch image classification model to ONNX format for deployment with OpenVINO Model Server.\n",
    "\n",
    "We use **SqueezeNet 1.1**, a lightweight convolutional neural network designed for efficient inference on resource-constrained environments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "Import PyTorch and torchvision for model loading and ONNX export."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "from pathlib import Path\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Torchvision version: {torchvision.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## 2. Load Pre-trained Model\n",
    "\n",
    "Load SqueezeNet 1.1 with weights pre-trained on ImageNet.\n",
    "\n",
    "**Model Details:**\n",
    "- Architecture: SqueezeNet 1.1\n",
    "- Input size: 224x224 RGB images\n",
    "- Output: 1000 ImageNet classes\n",
    "- Model size: ~5 MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained SqueezeNet 1.1 model\n",
    "model = models.squeezenet1_1(weights=models.SqueezeNet1_1_Weights.IMAGENET1K_V1)\n",
    "\n",
    "# Set model to evaluation mode (disables dropout, batch norm in training mode)\n",
    "model.eval()\n",
    "\n",
    "print(\"Model loaded successfully\")\n",
    "print(f\"Model architecture: SqueezeNet 1.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## 3. Create Output Directory\n",
    "\n",
    "Ensure the output directory exists for saving the ONNX model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "output_dir = Path(\"models/openvino\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "output_path = output_dir / \"model.onnx\"\n",
    "print(f\"Output path: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## 4. Export Model to ONNX Format\n",
    "\n",
    "Create a dummy input tensor matching the model's expected input shape and export to ONNX.\n",
    "\n",
    "**ONNX Export Parameters:**\n",
    "- `input_names`: Names for input tensors\n",
    "- `output_names`: Names for output tensors\n",
    "- `dynamic_axes`: Allow dynamic batch size for inference\n",
    "- `opset_version`: ONNX opset version (13 is compatible with OpenVINO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy input tensor (batch_size=1, channels=3, height=224, width=224)\n",
    "dummy_input = torch.randn(1, 3, 224, 224)\n",
    "\n",
    "# Export to ONNX format\n",
    "torch.onnx.export(\n",
    "    model,                          # Model to export\n",
    "    dummy_input,                    # Example input tensor\n",
    "    str(output_path),               # Output file path\n",
    "    input_names=[\"input\"],          # Input tensor name\n",
    "    output_names=[\"output\"],        # Output tensor name\n",
    "    dynamic_axes={\n",
    "        \"input\": {0: \"batch_size\"},  # Allow variable batch size\n",
    "        \"output\": {0: \"batch_size\"}\n",
    "    },\n",
    "    opset_version=13,               # ONNX opset version\n",
    "    export_params=True,             # Export trained parameters\n",
    "    do_constant_folding=True        # Optimize constant folding\n",
    ")\n",
    "\n",
    "print(f\"\\nModel exported successfully to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## 5. Verify ONNX Export\n",
    "\n",
    "Check that the ONNX file was created and display its size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify file exists and get size\n",
    "if output_path.exists():\n",
    "    file_size = output_path.stat().st_size\n",
    "    file_size_mb = file_size / (1024 * 1024)\n",
    "    print(f\"✓ ONNX model created successfully\")\n",
    "    print(f\"  File: {output_path}\")\n",
    "    print(f\"  Size: {file_size_mb:.1f} MB\")\n",
    "else:\n",
    "    print(\"✗ Error: ONNX file was not created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You have successfully:\n",
    "1. Loaded a pre-trained SqueezeNet 1.1 model from torchvision\n",
    "2. Exported the model to ONNX format with dynamic batch size support\n",
    "3. Saved the ONNX model to `models/openvino/model.onnx`\n",
    "\n",
    "The exported model is ready to be uploaded to S3 object storage and deployed using OpenVINO Model Server."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
