{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save and Upload a Model to S3\n",
    "\n",
    "Save a trained model in ONNX format and upload the file to an S3 bucket.\n",
    "\n",
    "### 1. Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Train a sample model\n",
    "\n",
    "Generate a random data set of normally-distributed data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X, _ = make_blobs(100, centers=1, n_features=2, random_state=0)\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "print(f\"Generated data has {X.shape[0]} samples and {X.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preview the data: display the first ten samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X[:10, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a simple autoencoder network for anomaly detection.\n",
    "\n",
    "* Autoencoders are designed to reconstruct the input data (X). Their output has the same shape as the input.\n",
    "* Autoencoders can filter anomalous data because its latent space forces the network to retain only relevant information.\n",
    "* You can detect anomalies by selecting the samples that the model fails to reconstruct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, num_features: int):\n",
    "        super(Model, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(num_features, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, 2)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(2, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(5, num_features)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        latent_space = self.encoder(x)\n",
    "        reconstructions = self.decoder(latent_space)\n",
    "        return reconstructions\n",
    "\n",
    "\n",
    "# Instantiate the model\n",
    "model = Model(num_features=X.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a model for anomaly detection by using the generated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function and the optimizer for back propagation\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Train the model during a few epochs\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    reconstructions = model(X)\n",
    "    loss = loss_function(reconstructions, X)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the model and compute the error between each sample and its reconstructions.\n",
    "The samples with relevant errors in its reconstructions are anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate anomalies\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    reconstructions = model(X)\n",
    "    errors = torch.mean((reconstructions - X) ** 2, axis=1)\n",
    "\n",
    "# Define threshold to detect 5% of the data as anomalies\n",
    "threshold = torch.quantile(errors, 0.95).item()\n",
    "# Find anomalies based on errors\n",
    "anomalies = (errors > threshold).int().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the results.\n",
    "The diagram displays anomalies in red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = np.array([\n",
    "    \"#00aa52\",  # Ok\n",
    "    \"#ef3300\",  # Anomaly\n",
    "])\n",
    "y_pred_colors = colors[anomalies]\n",
    "plt.scatter(X[:, 0], X[:, 1], color=y_pred_colors)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Export the trained model to ONNX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a single sample row for the model to infer the input shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_row = X[:1]\n",
    "sample_row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the PyTorch model to ONNX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_file_name = \"anomaly_detection.onnx\"\n",
    "torch.onnx.export(model, sample_row, onnx_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Upload the file to the S3 bucket.\n",
    "\n",
    "Use the `boto3` library with the parameters of the data connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_id = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "secret_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "endpoint = os.getenv(\"AWS_S3_ENDPOINT\")\n",
    "bucket_name = os.getenv(\"AWS_S3_BUCKET\")\n",
    "\n",
    "s3_client = boto3.client(\n",
    "    \"s3\",\n",
    "    aws_access_key_id=key_id,\n",
    "    aws_secret_access_key=secret_key,\n",
    "    endpoint_url=f\"http://{endpoint}\"  # boto3 requires the protocol\n",
    ")\n",
    "\n",
    "s3_client.upload_file(onnx_file_name, bucket_name, Key=onnx_file_name)\n",
    "\n",
    "print(f\"File {onnx_file_name} uploaded to S3!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open https://minio-ui-minio.apps.ocp4.example.com/.\n",
    "\n",
    "Authenticate with the `minio` access key and the `minio123` secret key.\n",
    "\n",
    "Verify that the `saved-models` bucket contains the ONNX model file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
