{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import required libraries and components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Read the data set that you generated in the previous notebook.\n",
    "For convenience, save the train and test subsets in CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"dataset.csv\")\n",
    "\n",
    "# Generate train and test splits\n",
    "train_data, test_data = train_test_split(\n",
    "    data,\n",
    "    test_size=0.2,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "train_data.to_csv(\"train_subset.csv\", index=False)\n",
    "test_data.to_csv(\"test_subset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Define the exact same data engineering function as in the preceding notebook to apply the same transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    variations = [df]\n",
    "    for i in range(10000):\n",
    "        variations.append(df.copy() * i)\n",
    "\n",
    "    df_extended = pd.concat(variations, axis=1)\n",
    "\n",
    "    return df_extended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Train the model by streaming the data from the file in chunks and preprocessing each chunk at once.\n",
    "By doing so, only some of the data needs to fit into memory at the same time.\n",
    "\n",
    "> NOTE You must use classifiers that work well with piecemeal, or \"out-of-core\", training.\n",
    "The `GaussianNB` classifier, or Gaussian naive-Bayes, is one such classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GaussianNB()\n",
    "\n",
    "batches = 0\n",
    "for minibatch in pd.read_csv(\"train_subset.csv\", chunksize=1000):\n",
    "    X = minibatch.drop('target', axis=1)\n",
    "    y = minibatch['target']\n",
    "    X_processed = preprocess(X)\n",
    "    model.partial_fit(X_processed, y, classes=y.unique())\n",
    "\n",
    "    batches += 1\n",
    "    print(f\"Training batch {batches} done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Test the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"test_subset.csv\")\n",
    "X_test = test_data.drop('target', axis=1)\n",
    "y_test = test_data['target']\n",
    "\n",
    "\n",
    "y_predicted = model.predict(preprocess(X_test))\n",
    "\n",
    "\n",
    "print(\"Classification report:\")\n",
    "print(classification_report(y_test, y_predicted))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
